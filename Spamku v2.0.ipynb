{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spamku \n",
    "## May 4, 2020\n",
    "## Version 2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Setup Variables and Define Helper Functions\n",
    "\n",
    "Import nltk libraries\n",
    "\n",
    "Setup dictionaries for storing learning.  Define a couple helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Nate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\Nate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# https://www.nltk.org/\n",
    "import nltk;\n",
    "\n",
    "# http://www.nltk.org/book/ch05.html\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('cmudict')\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "# https://docs.python.org/3/library/os.html\n",
    "import os\n",
    "\n",
    "# https://docs.python.org/3/library/pickle.html\n",
    "import pickle\n",
    "\n",
    "# https://docs.python.org/3/library/collections.html\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "pos_dict=defaultdict(list)\n",
    "# declare custom dictionary for determining part of speech\n",
    "pronunciation_dict = cmudict.dict()\n",
    "\n",
    "# https://docs.python.org/3/library/random.html\n",
    "import random\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "def clean_word(word):\n",
    "    word= word.translate(str.maketrans('', '', string.punctuation))\n",
    "    word=word.lower()\n",
    "    return word\n",
    "\n",
    "def get_syllable_count(word):\n",
    "    # print(pronunciation_dict[word])\n",
    "    if word == 'spamku':\n",
    "        return 2\n",
    "    else:\n",
    "        syl=[ x for x in pronunciation_dict[word][0] if x[-1].isdigit()]\n",
    "        return len(syl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Corpus of Sample Spam as a text file and also make a list of haiku, so each item in the list is a complete three line haiku\n",
    "\n",
    "This file contains 8893 spam-ku\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the name of the corpus is text. It is just one long string, no newlines\n",
    "filename=\"corpus/spamku.txt\"\n",
    "file = open(filename, mode = 'r')\n",
    "text = \"\"\n",
    "for line in file:\n",
    "    text += line.strip()+' '\n",
    "file.close()\n",
    "c=0\n",
    "l=[]\n",
    "# haiku_list is the name of our list which holds the haiku in their original form\n",
    "haiku_list=[]\n",
    "file = open(filename, mode = 'r')\n",
    "for line in file:\n",
    "    c+=1\n",
    "    l.append(line)\n",
    "    if c==3:\n",
    "        haiku_list.append(l)\n",
    "        c=0\n",
    "        l=[]\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The first 3 spamku:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['See the pretty SPAM\\n',\n",
       "  'Flying through the bedroom door\\n',\n",
       "  'Heading towards your face.\\n'],\n",
       " ['Ears, snouts and innards,\\n',\n",
       "  'A homogeneous mass--\\n',\n",
       "  'Pass another slice.\\n'],\n",
       " ['Pink tender morsel,\\n',\n",
       "  'Glistening with salty gel.\\n',\n",
       "  'What the hell is it?\\n']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haiku_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "### Read in pickle files \n",
    "\n",
    "pos_patterns is a dictionary with parts of speech patterns as keys, and how often that pattern occured in the corpus as values.  Each patterns is from one line of a haiku\n",
    "For example: 'JJ NN NN' : 140  \n",
    "  \n",
    "pos_dict is a dictionary with indivdual parts of speech as keys, and the value is a list of all words from our spamku corpus that match that part of speech  \n",
    "  \n",
    "These dictionaries take a while to generate on my i5-3320M computer, so I pickled them.  See the end of the notebook to see how I generated them (those cells will consist of commented out code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(filename): \n",
    "    file = open(filename, 'rb')      \n",
    "    data = pickle.load(file) \n",
    "    file.close()\n",
    "    return data\n",
    "\n",
    "pos_patterns = read_pickle(\"pos_patterns.p\")\n",
    "pos_dict = read_pickle(\"pos_dict.p\")\n",
    "pos_patterns5 = read_pickle(\"pos_patterns5.p\")\n",
    "pos_patterns7 = read_pickle(\"pos_patterns7.p\")\n",
    "pattern_syl5 = read_pickle(\"pattern_syl5.p\")\n",
    "pattern_syl7 = read_pickle(\"pattern_syl7.p\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pos_dict  -->  Key is a part of speech, value is a list of all words found having that part of speech.  Note duplicates are allowed.  This increases the probability of selecting common words when using random.choice to select a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['see', 'give', 'be', 'do', 'keep', 'have', 'be', 'do', 'do', 'do']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_dict['VB'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### pos_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Markov Chain Analysis on Sample Poems\n",
    "\n",
    "Determining the most likely word to follow each word in sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_markov_dict(text, filename):\n",
    "    \n",
    "    #text is a long string, filename is a file to save rejects to\n",
    "    f = open(filename, 'a')\n",
    "    # Tokenize the text by word, though including punctuation\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    # Initialize a default dictionary to hold all of the words and next words\n",
    "    markov_dict = defaultdict(list)\n",
    "    \n",
    "    # Create a zipped list of all of the word pairs and put them in word: list of next words format\n",
    "    for current_word, next_word in zip(words[0:-1], words[1:]):\n",
    "        try:\n",
    "            if get_syllable_count(clean_word(current_word)):\n",
    "                if get_syllable_count(clean_word(next_word)):\n",
    "                    markov_dict[clean_word(current_word)].append(clean_word(next_word) )    \n",
    "        except:\n",
    "            #f.write(clean_word(current_word)+'\\n')\n",
    "            print(clean_word(current_word), file=f)\n",
    "            pass\n",
    "    # Convert the default dict back into a dictionary\n",
    "    markov_dict = dict(markov_dict)\n",
    "    f.close()\n",
    "    return markov_dict\n",
    "\n",
    "mc=make_markov_dict(text, \"words_without_syllable_count.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11389"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(mc.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to count syllables for each word\n",
    "\n",
    "Testing with 3 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def get_syllable_count(word):\n",
    "    # print(pronunciation_dict[word])\n",
    "    if word == 'spamku':\n",
    "        return 2\n",
    "    else:\n",
    "        syl=[ x for x in pronunciation_dict[word][0] if x[-1].isdigit()]\n",
    "        return len(syl)\n",
    "\n",
    "print(get_syllable_count('pregnant'))\n",
    "print(get_syllable_count('sausage'))\n",
    "print(get_syllable_count('ribs'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to return part of speech for each word\n",
    "\n",
    "Testing with 3 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN\n",
      "NN\n",
      "NN\n"
     ]
    }
   ],
   "source": [
    "def get_pos_label(word):\n",
    "    return pos_tag(word)[0][1]\n",
    "\n",
    "print(get_pos_label(['capicola']))\n",
    "print(get_pos_label(['pastrami']))\n",
    "print(get_pos_label(['andouille']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create functions to choose POS pattern and syllable structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in get word pepto NN 3\n",
      "ham what makes \n",
      "the spam visit mom the spam \n",
      "error in get word porcine NN 2\n",
      "ham maybe not \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "       \n",
    "def get_word(pos, count, prevword):\n",
    "    for i in range(200):\n",
    "        try:\n",
    "            word = random.choice(mc[clean_word(prevword)]) if prevword else random.choice(pos_dict[pos])\n",
    "            if get_syllable_count(clean_word(word)) == count:\n",
    "                return clean_word(word)\n",
    "        except:\n",
    "            print('error in get word' , word, pos , count)\n",
    "            return(\"ham\")\n",
    "    return('spamku')\n",
    "#print(get_word('NN', 3, False))\n",
    "#print(get_word('NN', 2, 'spam'))\n",
    "def make_line(line_pattern, line_syl):\n",
    "    l=\"\"\n",
    "    prev_word=False\n",
    "    #print(\"make line: \", line_pattern, line_syl)\n",
    "    #print(\"length of  line pattern: \", len(line_pattern.split()), \"len of line syl: \", len(line_syl))\n",
    "    for pos, count in zip(line_pattern.split(), line_syl):\n",
    "        #print(\"I'm here:\", pos, count)\n",
    "        word = get_word(pos,count, prev_word)\n",
    "        l += word + ' '\n",
    "        prev_word = word\n",
    "    return l\n",
    "#print(make_line(firstlinepat, firstlinesyl))\n",
    "\n",
    "def get_line_info():\n",
    "    line_info =[]\n",
    "    for i in range(3):\n",
    "        if i != 1:\n",
    "            tempPat=random.choice(pos_patterns5)\n",
    "            line_info.append([tempPat, random.choice(pattern_syl5[tempPat])])\n",
    "        else:\n",
    "            tempPat=random.choice(pos_patterns7)\n",
    "            line_info.append([tempPat, random.choice(pattern_syl7[tempPat])])\n",
    "    return line_info\n",
    "\n",
    "\n",
    "\n",
    "line_info = get_line_info()\n",
    "for line in line_info:\n",
    "    print(make_line(line[0], line[1]))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uh-oh\n"
     ]
    }
   ],
   "source": [
    "if 'pepto' in pos_dict['NN']:\n",
    "    print('uh-oh')\n",
    "else:\n",
    "    print(\"safe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process each line of sample\n",
    "\n",
    "Determine the arrangement of syllables\n",
    "\n",
    "Determine the parts of speech of each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick a pattern for each line\n",
    "\n",
    "Choose a syllable sequence for the 1st, 2nd and 3rd line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed the poems 1st word of syllable length from pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use chain to pick next word with correct number of syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spam', 'bedroom', 'door', 'face', 'mass', 'pass', 'slice', 'pink', 'tender', 'morsel', 'salty', 'gel']\n"
     ]
    }
   ],
   "source": [
    "print(pos_dict['NN'][:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions used to create pickled dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make pos patterns from haiku\n",
    "# pos_patterns =  defaultdict(list)\n",
    "# pos_patterns_markov_data = []\n",
    "# haiku_pat = []\n",
    "# for haiku in haiku_list:\n",
    "#     for line in haiku:\n",
    "#         l=\"\"\n",
    "#         line = line.strip()\n",
    "#         for word in line.split():\n",
    "#             word = clean_word(word)\n",
    "#             try:\n",
    "#                 l += pos_tag([word])[0][1]+\" \"\n",
    "#             except:\n",
    "#                 #I discovered that word is \"\" sometimes, which raises an error\n",
    "#                 #print(\"error when building l\", word)\n",
    "#                 pass\n",
    "#         haiku_pat.append(l)\n",
    "#         if l in pos_patterns:\n",
    "#             pos_patterns[l] += 1\n",
    "#         else:\n",
    "#             pos_patterns[l] = 1\n",
    "#     pos_patterns_markov_data.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make pos pattern - syllable information dict\n",
    "pos_patterns5 =  []\n",
    "pos_patterns7 =  []\n",
    "pattern_syl5 = defaultdict(list)\n",
    "pattern_syl7 = defaultdict(list)\n",
    "#pos_patterns_syl_markov_data_syl = []\n",
    "for haiku in haiku_list:\n",
    "    for line in haiku:\n",
    "        \n",
    "        l=\"\"\n",
    "        syl_pattern = []\n",
    "        line = line.strip()\n",
    "        for word in line.split():\n",
    "            word = clean_word(word)\n",
    "            try:\n",
    "                l += pos_tag([word])[0][1]+\" \"\n",
    "                syl_pattern.append(get_syllable_count(word))\n",
    "            except:\n",
    "                #I discovered that word is \"\" sometimes, which raises an error\n",
    "                #print(\"error when building l\", word)\n",
    "                pass\n",
    "        if sum(syl_pattern) == 5:\n",
    "            pattern_syl5[l].append(syl_pattern)\n",
    "            pos_patterns5.append(l)\n",
    "        if sum(syl_pattern) == 7:\n",
    "            pattern_syl7[l].append(syl_pattern)\n",
    "            pos_patterns7.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam i will he ate \n",
      "low on my friend i perform \n",
      "blew it taste not no \n",
      "\n",
      "aisle i force to your \n",
      "cans of hideous spamku \n",
      "three and fart in this \n",
      "\n",
      "burst your pork taste charles \n",
      "gotta have no food was no \n",
      "king of tang viscous \n",
      "\n",
      "spam dies on the vibe \n",
      "mc error from preword:  helpings\n",
      "\n",
      "Error.  Restarting...\n",
      "\n",
      "me i heed spamku \n",
      "what was wrong with the fridge one \n",
      "spamku i \n",
      "\n",
      "if spam te spamku whats \n",
      "freud spamku in the desert \n",
      "the toilet it is \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "       \n",
    "def get_word(pos, count, prevword):\n",
    "    for i in range(200):\n",
    "        try:\n",
    "            word = random.choice(mc[clean_word(prevword)]) if prevword else random.choice(pos_dict[pos])\n",
    "            if get_syllable_count(clean_word(word)) == count:\n",
    "                return clean_word(word)\n",
    "        except:\n",
    "            print(\"mc error from preword: \", prevword)\n",
    "            print('nate' , word, pos , count)\n",
    "            return(\"ham\")\n",
    "    return('spamku')\n",
    "#print(get_word('NN', 3, False))\n",
    "#print(get_word('NN', 2, 'spam'))\n",
    "def make_line(line_pattern, line_syl):\n",
    "    l=\"\"\n",
    "    prev_word=False\n",
    "    #print(\"make line: \", line_pattern, line_syl)\n",
    "    #print(\"length of  line pattern: \", len(line_pattern.split()), \"len of line syl: \", len(line_syl))\n",
    "    for pos, count in zip(line_pattern.split(), line_syl):\n",
    "        #print(\"I'm here:\", pos, count)\n",
    "        word = get_word(pos,count, prev_word)\n",
    "        l += word + ' '\n",
    "        prev_word = word\n",
    "    return l\n",
    "#print(make_line(firstlinepat, firstlinesyl))\n",
    "\n",
    "def get_line_info():\n",
    "    line_info =[]\n",
    "    for i in range(3):\n",
    "        if i != 1:\n",
    "            tempPat=random.choice(pos_patterns5)\n",
    "            line_info.append([tempPat, random.choice(pattern_syl5[tempPat])])\n",
    "        else:\n",
    "            tempPat=random.choice(pos_patterns7)\n",
    "            line_info.append([tempPat, random.choice(pattern_syl7[tempPat])])\n",
    "    return line_info\n",
    "\n",
    "def generate_spamku(num_wanted):\n",
    "    try:\n",
    "        for i in range(num_wanted):\n",
    "            line_info = get_line_info()\n",
    "            for line in line_info:\n",
    "                print(make_line(line[0], line[1]))\n",
    "            print()\n",
    "    except:\n",
    "        print(\"\\nError.  Restarting...\\n\")\n",
    "        generate_spamku(num_wanted-i)\n",
    "        pass\n",
    "              \n",
    "\n",
    "generate_spamku(5)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tricky pink greasy \n",
      "when pigs fly fish pig lagoons \n",
      "wont touch this spam a \n",
      "\n",
      "nine days i wonder \n",
      "well later postal i want \n",
      "sham has no fuzzy \n",
      "\n",
      "blue can as a man \n",
      "luscious spamku has spam and \n",
      "piggy cry greasy \n",
      "\n",
      "hockey spamku ballpoint \n",
      "spam has problems large snake was \n",
      "heart million swine were \n",
      "\n",
      "to knowledge spamku \n",
      "see next patient ms spam cans \n",
      "life turned to suffer \n",
      "\n",
      "for your spam maybe \n",
      "while eating spam crust first cold \n",
      "tin of spam them some \n",
      "\n",
      "particular i \n",
      "raw bimbo i was really \n",
      "hanging out excess \n",
      "\n",
      "here in the full of \n",
      "annoying spamku sure you \n",
      "thou hid her spam spam \n",
      "\n",
      "flies what the long too \n",
      "i ever understand you \n",
      "honey flowing to \n",
      "\n",
      "demented spam melt \n",
      "joke for reading i have you \n",
      "pink spamku \n",
      "\n",
      "hungry cry out for \n",
      "just trailer park trash saddam \n",
      "whole masters spamku \n",
      "\n",
      "love alone with food \n",
      "what day and south west north east \n",
      "a one more vile than \n",
      "\n",
      "to explain to do \n",
      "matter what is heavily \n",
      "just sitting at night \n",
      "\n",
      "youth charged chuck spamku \n",
      "i lather spamku is the \n",
      "lies palatable \n",
      "\n",
      "bubba jesus and \n",
      "pig meat in a spam i said \n",
      "heaps of the condoms \n",
      "\n",
      "me is on toast for \n",
      "the spam only one with spam \n",
      "a weapon called spam \n",
      "\n",
      "them a pig the star \n",
      "his dong spamku dates spamku \n",
      "down her spam and spam \n",
      "\n",
      "say head and gaelic \n",
      "air and still the track for the \n",
      "cellucci a pigs \n",
      "\n",
      "on spam and then what \n",
      "unlike most likely tolls for \n",
      "food many times now \n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'word' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-98acaed23b09>\u001b[0m in \u001b[0;36mget_word\u001b[1;34m(pos, count, prevword)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclean_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprevword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mprevword\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mget_syllable_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ein'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-636dbb4e9ce0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenerate_spamku\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-98acaed23b09>\u001b[0m in \u001b[0;36mgenerate_spamku\u001b[1;34m(num_wanted)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mline_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_line_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmake_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-98acaed23b09>\u001b[0m in \u001b[0;36mmake_line\u001b[1;34m(line_pattern, line_syl)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline_pattern\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline_syl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m#print(\"I'm here:\", pos, count)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0ml\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mprev_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-98acaed23b09>\u001b[0m in \u001b[0;36mget_word\u001b[1;34m(pos, count, prevword)\u001b[0m\n\u001b[0;32m      8\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mclean_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nate'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ham\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spamku'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'word' referenced before assignment"
     ]
    }
   ],
   "source": [
    "generate_spamku(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fingers dance and flushed \n",
      "of spam gods angel sing the \n",
      "fish bait the little \n",
      "\n",
      "eat one expected \n",
      "hanging spamku makes good \n",
      "battle then go to \n",
      "\n",
      "thirteen months mom the \n",
      "a whiff spamku all hang spam \n",
      "with spam is maps tell \n",
      "\n",
      "bad hair tin bong jim \n",
      "still looks like spam gay pink and \n",
      "my ear you squealing \n",
      "\n",
      "hah while i shot the \n",
      "vodka deaths pigs take it to \n",
      "beautiful weather \n",
      "\n",
      "pork bits of the milk \n",
      "says spam lump oozing pleasant \n",
      "you can fit for years \n",
      "\n",
      "wag the hormel spam \n",
      "i await you spam oh if \n",
      "square loaf twentieth \n",
      "\n",
      "conservation of \n",
      "you to fly in can give me \n",
      "dick where is truly \n",
      "\n",
      "and thong spam can of \n",
      "spam in a real bad but she \n",
      "reproduces spam \n",
      "\n",
      "public pinched a man \n",
      "feeling very good its not \n",
      "mountain the third world \n",
      "\n",
      "comfortable sleep \n",
      "crime was a can and spam is \n",
      "kisses are meat oh \n",
      "\n",
      "bile are you the can \n",
      "spam trails spamku of tasting \n",
      "you know their mentors \n",
      "\n",
      "taken spamku all \n",
      "of money consume only \n",
      "to glory i fry \n",
      "\n",
      "blew the pink goo wake \n",
      "santa left of any place \n",
      "to call unwanted \n",
      "\n",
      "my pillow what is \n",
      "of spamku along \n",
      "just see spot spamku \n",
      "\n",
      "job skills are belong \n",
      "some of salt tears through patient \n",
      "john gods smile war we \n",
      "\n",
      "please in a little \n",
      "new follicle life have it \n",
      "shelf stares and vomit \n",
      "\n",
      "desert life spam this \n",
      "hundred years no spam instead \n",
      "spam reminds me rich \n",
      "\n",
      "yeast now time set free \n",
      "lust the hell below at my \n",
      "and a can become \n",
      "\n",
      "sweet pink spam here eat \n",
      "it a pork nor guts baby \n",
      "for hormel but not \n",
      "\n",
      "spamku \n",
      "shades of chance to sit dear you \n",
      "mouth i knew spam spam \n",
      "\n",
      "that the pink i love \n",
      "god flush the bacteria \n",
      "can looks the rest on \n",
      "\n",
      "is china strikes out \n",
      "husband loves us reeking of \n",
      "some spam milk and bream \n",
      "\n",
      "of spam can be at \n",
      "stuff phlegmatic spamku a square \n",
      "and moose oh holy \n",
      "\n",
      "new land of love friends \n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'word' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-98acaed23b09>\u001b[0m in \u001b[0;36mget_word\u001b[1;34m(pos, count, prevword)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclean_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprevword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mprevword\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mget_syllable_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'collards'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-636dbb4e9ce0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenerate_spamku\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-98acaed23b09>\u001b[0m in \u001b[0;36mgenerate_spamku\u001b[1;34m(num_wanted)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mline_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_line_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmake_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-98acaed23b09>\u001b[0m in \u001b[0;36mmake_line\u001b[1;34m(line_pattern, line_syl)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline_pattern\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline_syl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m#print(\"I'm here:\", pos, count)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0ml\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mprev_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-98acaed23b09>\u001b[0m in \u001b[0;36mget_word\u001b[1;34m(pos, count, prevword)\u001b[0m\n\u001b[0;32m      8\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mclean_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nate'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ham\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spamku'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'word' referenced before assignment"
     ]
    }
   ],
   "source": [
    "generate_spamku(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'fading'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-5ff460da4148>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fading'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'fading'"
     ]
    }
   ],
   "source": [
    "mc['fading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_syllable_count('fading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it and be a just \n",
      "murder bought spamku sex pressed \n",
      "clinton repeat spam \n",
      "\n",
      "saves nine spam the road \n",
      "mc error from preword:  olde\n",
      "\n",
      "Error.  Restarting...\n",
      "as i must vote for \n",
      "pork cold running spamku \n",
      "it in space food real \n",
      "\n",
      "i thrust spam loaves and \n",
      "created for cherry tree \n",
      "spam is hard workers \n",
      "\n",
      "in spam when shadows \n",
      "and peas high upon my yeast \n",
      "altitude spam and \n",
      "\n",
      "spam meat heavy and \n",
      "sweet spam spam cans on top with \n",
      "spam cold enough spam \n",
      "\n",
      "anticipates the \n",
      "beantown e f loser must \n",
      "all boulder on my \n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_spamku(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and pork in a time \n",
      "fried in the net greasy pink \n",
      "the virtue of spam \n",
      "\n",
      "my pillow marks the \n",
      "instead spamku \n",
      "spam man has no pane \n",
      "\n",
      "always lick spam suit \n",
      "loser mistaken spamku \n",
      "meaty and let the \n",
      "\n",
      "eat it at me spam \n",
      "sticky spamku star wars have \n",
      "table no more sense \n",
      "\n",
      "silent suspicions \n",
      "headlines state zoning spamku \n",
      "not a ship lands with \n",
      "\n",
      "until you getting \n",
      "what the room for meat the spam \n",
      "like to the farm while \n",
      "\n",
      "spam it for midnight \n",
      "one two in a lump of spam \n",
      "cough spamku reach for \n",
      "\n",
      "me with de feet am \n",
      "how do you for revenge spam \n",
      "oneness wet poop is \n",
      "\n",
      "repulsive is maps \n",
      "larry lowe spamku i can \n",
      "the john open the \n",
      "\n",
      "he blows close walden \n",
      "with pink soldiers in the moon \n",
      "bowel tornado \n",
      "\n",
      "feed from blue grass spam \n",
      "their really knows answer was \n",
      "enigmatic box \n",
      "\n",
      "it twitches slightly \n",
      "they live love and ends spamku \n",
      "step in a pink guy \n",
      "\n",
      "pink meat treat open \n",
      "ish spamku no more spam morning \n",
      "are just find new hit \n",
      "\n",
      "will spam spam its a \n",
      "dirty ape butler spamku \n",
      "lube for spam ask me \n",
      "\n",
      "me so good service \n",
      "pigs sing along spamku \n",
      "pit bull wrongfully \n",
      "\n",
      "world the spam the bin \n",
      "can bulges grab spamku \n",
      "sing your ass cum on \n",
      "\n",
      "are you with a meat \n",
      "pies in god for coffin no \n",
      "the poor rotting fish \n",
      "\n",
      "lonely night calling \n",
      "monk spam babe pool tempting smell \n",
      "green spamku \n",
      "\n",
      "you i mean the warm \n",
      "mc error from preword:  fading\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'word' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-a3d1af13b5f0>\u001b[0m in \u001b[0;36mget_word\u001b[1;34m(pos, count, prevword)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclean_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprevword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mprevword\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mget_syllable_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'fading'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-636dbb4e9ce0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenerate_spamku\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-51-a3d1af13b5f0>\u001b[0m in \u001b[0;36mgenerate_spamku\u001b[1;34m(num_wanted)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mline_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_line_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmake_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-a3d1af13b5f0>\u001b[0m in \u001b[0;36mmake_line\u001b[1;34m(line_pattern, line_syl)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline_pattern\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline_syl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m#print(\"I'm here:\", pos, count)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0ml\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mprev_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-a3d1af13b5f0>\u001b[0m in \u001b[0;36mget_word\u001b[1;34m(pos, count, prevword)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mc error from preword: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nate'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ham\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spamku'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'word' referenced before assignment"
     ]
    }
   ],
   "source": [
    "generate_spamku(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 ive\n",
      "10000 to\n",
      "15000 bill\n",
      "20000 western\n",
      "25000 below\n",
      "30000 deadly\n",
      "35000 ate\n",
      "40000 godzilla\n",
      "45000 was\n",
      "50000 their\n",
      "55000 the\n",
      "60000 rises\n",
      "65000 love\n",
      "70000 were\n",
      "75000 stock\n",
      "80000 stronger\n",
      "85000 pan\n",
      "90000 a\n",
      "95000 an\n",
      "100000 i\n",
      "105000 ammo\n",
      "110000 blend\n",
      "***************  Done  ***************\n"
     ]
    }
   ],
   "source": [
    "# Make pos_dict from spamku\n",
    "\n",
    "import string\n",
    "import sys\n",
    "c=0\n",
    "pos_dict=defaultdict(list)\n",
    "for word in text.split(' '):\n",
    "    try:\n",
    "        c+=1\n",
    "        word = clean_word(word) \n",
    "        if c % 5000 == 0:\n",
    "            print(c, word)\n",
    "        tok = pos_tag([word])\n",
    "        # This line is here to throw an error if we don't have a syllable count for this word\n",
    "        syl = get_syllable_count(word)\n",
    "        pos_dict[tok[0][1]].append(tok[0][0])\n",
    "    except:\n",
    "        #print(\"SYL ERROR\", tok, word)\n",
    "        continue\n",
    "print('***************  Done  ***************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "safe\n"
     ]
    }
   ],
   "source": [
    "if 'porcine' in pos_dict['NN']:\n",
    "    print('uh-oh')\n",
    "else:\n",
    "    print(\"safe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pos_dict, open(\"pos_dict.p\", \"wb\"))\n",
    "pickle.dump(pos_patterns, open(\"pos_patterns.p\", \"wb\"))\n",
    "pickle.dump(pos_patterns5, open(\"pos_patterns5.p\", \"wb\"))\n",
    "pickle.dump(pos_patterns7, open(\"pos_patterns7.p\", \"wb\"))\n",
    "pickle.dump(pattern_syl5, open(\"pattern_syl5.p\", \"wb\"))\n",
    "pickle.dump(pattern_syl7, open(\"pattern_syl7.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos_dict['NN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNfreq = {x:pos_dict['NN'].count(x) for x in pos_dict['NN']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NNfreqKeys = list(NNfreq.keys())\n",
    "#NNfreqKeys[:20]\n",
    "NNfreq['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print butt\n",
    "except:\n",
    "    print(err)\n",
    "    print(\"in except block\")\n",
    "    pass\n",
    "print(burt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> a = [1,1,1,1,2,2,2,2,3,3,4,5,5]\n",
    ">>> d = {x:a.count(x) for x in a}\n",
    ">>> d\n",
    "{1: 4, 2: 4, 3: 2, 4: 1, 5: 2}\n",
    ">>> a, b = d.keys(), d.values()\n",
    ">>> a\n",
    "[1, 2, 3, 4, 5]\n",
    ">>> b\n",
    "[4, 4, 2, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(word):\n",
    "    word= word.translate(str.maketrans('', '', string.punctuation))\n",
    "    word=word.lower()\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=open(\"rejected_words.py\", 'r')\n",
    "# rejects=[]\n",
    "# for word in f:\n",
    "#     word = clean_word(word)\n",
    "#     rejects.append(word)\n",
    "# rejects=set(rejects)\n",
    "# f.close()\n",
    "# f=open(\"clean_rejects.txt\", 'a')\n",
    "# for word in rejects:\n",
    "#     f.write(word.strip()+':  ,\\n')\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern_prob = []\n",
    "# for k,v in pos_patterns.items():\n",
    "#     for i in range(v):\n",
    "#         try:\n",
    "#             if v != \"\":\n",
    "#                 pattern_prob.append(k)\n",
    "#         except:\n",
    "#             print(\"error: \", k, v)\n",
    "# num_diff_patterns = len(pattern_prob)\n",
    "\n",
    "# syllable_lengths = {\n",
    "#     'VB': 1,\n",
    "#     'DT': 1,\n",
    "#     'RB': 2,\n",
    "#     'NN': 5,\n",
    "#     'VBG': 3,\n",
    "#     'IN': 1,\n",
    "#     'NNS': 4,\n",
    "#     'PRP$': 1,\n",
    "#     'CC': 1,\n",
    "#     'JJ': 2,\n",
    "#     'WP': 1,\n",
    "#     'VBZ': 1,\n",
    "#     'PRP': 1, \n",
    "#     'VBN': 2,\n",
    "#     'JJS': 2,\n",
    "#     'MD': 1, \n",
    "#     'TO': 1,\n",
    "#     'VBD': 2,\n",
    "#     'VBP': 2,\n",
    "#     'WRB': 1,\n",
    "#     'CD': 1, #cardinal digit, could be more\n",
    "#     'RBR': 2,\n",
    "#     'JJR': 2,\n",
    "#     'WDT': 1,\n",
    "#     'WP$': 1\n",
    "# }\n",
    "\n",
    "\n",
    "# # Maybe do a probability analysis on these?\n",
    "# # CC coordinating conjunction\n",
    "# # CD cardinal digit\n",
    "# # DT determiner\n",
    "# # EX existential there (like: there is  think of it like there exists)\n",
    "# # FW foreign word\n",
    "# # IN preposition/subordinating conjunction\n",
    "# # JJ adjective big\n",
    "# # JJR adjective, comparative bigger\n",
    "# # JJS adjective, superlative biggest\n",
    "# # LS list marker 1)\n",
    "# # MD modal could, will\n",
    "# # NN noun, singular desk\n",
    "# # NNS noun plural desks\n",
    "# # NNP proper noun, singular Harrison\n",
    "# # NNPS proper noun, plural Americans\n",
    "# # PDT predeterminer all the kids\n",
    "# # POS possessive ending parents\n",
    "# # PRP personal pronoun I, he, she\n",
    "# # PRP$ possessive pronoun my, his, hers\n",
    "# # RB adverb very, silently,\n",
    "# # RBR adverb, comparative better\n",
    "# # RBS adverb, superlative best\n",
    "# # RP particle give up\n",
    "# # TO, to go to the store.\n",
    "# # UH interjection, errrrrrrrm\n",
    "# # VB verb, base form take\n",
    "# # VBD verb, past tense took\n",
    "# # VBG verb, gerund/present participle taking\n",
    "# # VBN verb, past participle taken\n",
    "# # VBP verb, sing. present, non-3d take\n",
    "# # VBZ verb, 3rd person sing. present takes\n",
    "# # WDT wh-determiner which\n",
    "# # WP wh-pronoun who, what\n",
    "# # WP$ possessive wh-pronoun whose\n",
    "# # WRB wh-abverb where, when\n",
    "\n",
    "# def choose_target_syllable_count(pattern, number_of_syllables_wanted):\n",
    "#     syllable_counts = []\n",
    "#     possible = False\n",
    "#     pattern_len = len(pattern.split())\n",
    "#     max_per_word = number_of_syllables_wanted - pattern_len\n",
    "#     for i in range(len(pattern.split())):\n",
    "#         while possible == False:\n",
    "#             x = randint(1, syllable_lengths[pattern.split()[i]])\n",
    "#             print(x)\n",
    "#             if x <= max_per_word:\n",
    "#                 possible = True\n",
    "#         syllable_counts.append(x)\n",
    "#         number_of_syllables_wanted -= x\n",
    "#         pattern_len -= 1\n",
    "#         max_per_word = number_of_syllables_wanted - pattern_len\n",
    "#         print('***********************', syllable_counts)\n",
    "#     return syllable_counts\n",
    "\n",
    "# #pass in poem, pattern, \n",
    "                    \n",
    "# def build_line(line, pattern, max_length):\n",
    "#     if len(pattern) == 0:\n",
    "#         return line\n",
    "#     else:\n",
    "#         pos = pattern[-1]\n",
    "#     try:\n",
    "#         new_max_length = max_length\n",
    "#         #print(max_length)\n",
    "#         word = random.choice(pos_dict[pos])\n",
    "#         new_max_length = max_length - get_syllable_count(word)\n",
    "#         if new_max_length >= 0:\n",
    "#             line = word + ' ' + line\n",
    "#         else:\n",
    "#             new_max_length = max_length\n",
    "#     except:\n",
    "#         print(\"funny word: \", word)        \n",
    "#     #if max_length >= 0:\n",
    "#     pattern=pattern[:-1]\n",
    "#     return build_line(line, pattern, new_max_length)\n",
    "\n",
    "## word1.capitalize()        \n",
    "        \n",
    "# line1text=\"\"\n",
    "# word=\"\"\n",
    "# c=0\n",
    "# for pos in line1:\n",
    "#     word=\"\"\n",
    "#     possible = False\n",
    "#     try:\n",
    "#         while possible == False:\n",
    "#             word = random.choice(pos_dict[pos])\n",
    "#             if get_syllable_count(word) = l1[c]\n",
    "#                 line1text += word \" \"\n",
    "#                 c += 1\n",
    "#                 possible = true\n",
    "#         except:\n",
    "#             print(\"Unexpected error when adding to dict:\", sys.exc_info()[0])\n",
    "#             print(\"pos:\", pos, l1[c], word)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
