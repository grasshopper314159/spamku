{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spamku \n",
    "The objective is to write haiku about Spam (ala Hormel) based on a corpus of \"spamku\" from the \"SPAM Haiku Archive Home Page - MIT\" at http://web.mit.edu/jync/www/spam/ > There are about 20,000 but I used the 9000 or so ones that were curated into various \"best of\" categories.\n",
    "\n",
    "Words are chosen with three criteria in mind.  First every line in corpus was analyzed for parts of speech and put into a list.  Duplicates are allowed.  A part of speech pattern is chosen from this list first. Second, a syllable pattern is chosen from a dictionary that has all observed syllable patterns for that part-of-speech pattern.   Next, a word is chosen from a markov chain (obviously the first word is random).  If the word doesn't match the part of speech, another is chosen until it does.  Next that word is checked against the syllable pattern.  If it doesn't match, the process begins again until a word that matches part of speech and syllable pattern is found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Setup Variables and Define Helper Functions\n",
    "\n",
    "Import nltk libraries\n",
    "\n",
    "Setup dictionaries for storing learning.  Define a couple helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Nate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\Nate\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# https://www.nltk.org/\n",
    "import nltk;\n",
    "\n",
    "# http://www.nltk.org/book/ch05.html\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('cmudict')\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "# https://docs.python.org/3/library/os.html\n",
    "import os\n",
    "\n",
    "# https://docs.python.org/3/library/pickle.html\n",
    "import pickle\n",
    "\n",
    "# https://docs.python.org/3/library/collections.html\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "pos_dict=defaultdict(list)\n",
    "# declare custom dictionary for determining part of speech\n",
    "pronunciation_dict = cmudict.dict()\n",
    "\n",
    "# https://docs.python.org/3/library/random.html\n",
    "import random\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "def clean_word(word):\n",
    "    word= word.translate(str.maketrans('', '', string.punctuation))\n",
    "    word=word.lower()\n",
    "    return word\n",
    "\n",
    "def get_syllable_count(word):\n",
    "    # print(pronunciation_dict[word])\n",
    "    if word == 'spamku':\n",
    "        return 2\n",
    "    else:\n",
    "        syl=[ x for x in pronunciation_dict[word][0] if x[-1].isdigit()]\n",
    "        return len(syl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Corpus of Sample Spam as a text file and also make a list of haiku, so each item in the list is a complete three line haiku\n",
    "\n",
    "This file contains 8893 spam-ku\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the name of the corpus is text. It is just one long string, no newlines\n",
    "filename=\"corpus/spamku.txt\"\n",
    "file = open(filename, mode = 'r')\n",
    "text = \"\"\n",
    "for line in file:\n",
    "    text += line.strip()+' '\n",
    "file.close()\n",
    "\n",
    "\n",
    "c=0\n",
    "l=[]\n",
    "# haiku_list is the name of our list which holds the haiku in their original form\n",
    "haiku_list=[]\n",
    "file = open(filename, mode = 'r')\n",
    "for line in file:\n",
    "    c+=1\n",
    "    l.append(line)\n",
    "    if c==3:\n",
    "        haiku_list.append(l)\n",
    "        c=0\n",
    "        l=[]\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The first 3 spamku:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['See the pretty SPAM\\n',\n",
       "  'Flying through the bedroom door\\n',\n",
       "  'Heading towards your face.\\n'],\n",
       " ['Ears, snouts and innards,\\n',\n",
       "  'A homogeneous mass--\\n',\n",
       "  'Pass another slice.\\n'],\n",
       " ['Pink tender morsel,\\n',\n",
       "  'Glistening with salty gel.\\n',\n",
       "  'What the hell is it?\\n']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haiku_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "### Read in pickle files \n",
    "\n",
    "pos_dict is a dictionary with indivdual parts of speech as keys, and the value is a list of all words from our spamku corpus that match that part of speech.  It is filtered so only words that work with our syllable count function are used.\n",
    "\n",
    "pos_patterns is a dictionary with parts-of-speech-patterns as keys, and how often that pattern occured in the corpus as values.  Each patterns is from one line of a haiku\n",
    "For example: 'JJ NN NN' : 140  It's not really used except as an interesting statistic.\n",
    "\n",
    "Instead I use pos_patters5 and pos_patterns7, which are just lists of parts-of-speech-patterns, separated by whether they occured in a five or seven syllable line.  It allows duplicates, so common patterns are chosen more frequently.\n",
    "\n",
    "pattern_syl5 and pattern_syl7 are dictionaries with parts_of_speech_patterns as keys.  The value is a list containing all syllable counts associated with the pattern.  For example: \n",
    "pattern_syl7 \\[ 'VBG IN NN NN ' \\] = \\[ \\[ 3, 1, 2, 1 \\], \\[ 2, 2, 1, 2 \\], \\[ 2, 1, 2, 2 \\] \\]\n",
    "\n",
    "Some of these dictionaries take a while to generate on my i5-3320M computer, so I pickled them.  See the end of the notebook to see how I generated them (those cells will consist of commented out code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(filename): \n",
    "    file = open(filename, 'rb')      \n",
    "    data = pickle.load(file) \n",
    "    file.close()\n",
    "    return data\n",
    "\n",
    "pos_patterns = read_pickle(\"pos_patterns.p\")\n",
    "pos_dict = read_pickle(\"pos_dict.p\")\n",
    "pos_patterns5 = read_pickle(\"pos_patterns5.p\")\n",
    "pos_patterns7 = read_pickle(\"pos_patterns7.p\")\n",
    "pattern_syl5 = read_pickle(\"pattern_syl5.p\")\n",
    "pattern_syl7 = read_pickle(\"pattern_syl7.p\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Markov Chain Analysis on Sample Poems\n",
    "\n",
    "Determining the most likely word to follow each word in sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_markov_dict(text, filename):\n",
    "    \n",
    "    #text is a long string, filename is a file to save rejects to\n",
    "    f = open(filename, 'a')\n",
    "    # Tokenize the text by word, though including punctuation\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    # Initialize a default dictionary to hold all of the words and next words\n",
    "    markov_dict = defaultdict(list)\n",
    "    \n",
    "    # Create a zipped list of all of the word pairs and put them in word: list of next words format\n",
    "    for current_word, next_word in zip(words[0:-1], words[1:]):\n",
    "        try:\n",
    "            if get_syllable_count(clean_word(current_word)):  # throws an error if we don't have a syllable count\n",
    "                if get_syllable_count(clean_word(next_word)): # throws an error if we don't have a syllable count\n",
    "                if get_syllable_count(clean_word(next_word)):\n",
    "                    markov_dict[clean_word(current_word)].append(clean_word(next_word) )    \n",
    "        except:\n",
    "            # saves words without syllable count to a file.  I can manually add these later\n",
    "            print(clean_word(current_word), file=f)\n",
    "            pass\n",
    "    # Convert the default dict back into a dictionary\n",
    "    markov_dict = dict(markov_dict)\n",
    "    f.close()\n",
    "    return markov_dict\n",
    "\n",
    "mc=make_markov_dict(text, \"words_without_syllable_count.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11389"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(mc.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to count syllables for each word\n",
    "\n",
    "Testing with 3 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def get_syllable_count(word):\n",
    "    # print(pronunciation_dict[word])\n",
    "    if word == 'spamku':\n",
    "        return 2\n",
    "    else:\n",
    "        syl=[ x for x in pronunciation_dict[word][0] if x[-1].isdigit()]\n",
    "        return len(syl)\n",
    "\n",
    "print(get_syllable_count('pregnant'))\n",
    "print(get_syllable_count('sausage'))\n",
    "print(get_syllable_count('ribs'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to return part of speech for each word\n",
    "\n",
    "Testing with 3 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN\n",
      "NN\n",
      "NN\n"
     ]
    }
   ],
   "source": [
    "def get_pos_label(word):\n",
    "    return pos_tag(word)[0][1]\n",
    "\n",
    "print(get_pos_label(['capicola']))\n",
    "print(get_pos_label(['pastrami']))\n",
    "print(get_pos_label(['andouille']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create functions to choose POS pattern and syllable structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue can coat your face \n",
      "with it reminds me turkey \n",
      "enlightened in the \n",
      "\n",
      "wild west virginia \n",
      "down at birth control spittle \n",
      "your lunch prepare eat \n",
      "\n",
      "eat it take it looks \n",
      "it delight ah mismanaged \n",
      "really got it for \n",
      "\n",
      "the litter next to \n",
      "the sun has it is always \n",
      "gaseous clouds and \n",
      "\n",
      "but there they always \n",
      "things we part the sign of night \n",
      "but man dressed spam \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "       \n",
    "def get_word(pos, count, prevword):\n",
    "    for i in range(2000): #tries up to 2000 times to get a good word\n",
    "        try:\n",
    "            word = random.choice(mc[clean_word(prevword)]) if prevword else random.choice(pos_dict[pos])\n",
    "            if get_syllable_count(clean_word(word)) == count:\n",
    "                return clean_word(word)\n",
    "        except: #A bug, sometimes a word is chosen that can't be matched in the markov chain...\n",
    "            print(\"mc error from preword: \", prevword)\n",
    "            print('nate' , word, pos , count)\n",
    "            return(\"ham\")\n",
    "    return('spam') #if I can't get a word in 2000 tries\n",
    "\n",
    "def make_line(line_pattern, line_syl):\n",
    "    l=\"\"\n",
    "    prev_word=False\n",
    "    #print(\"make line: \", line_pattern, line_syl)\n",
    "    #print(\"length of  line pattern: \", len(line_pattern.split()), \"len of line syl: \", len(line_syl))\n",
    "    for pos, count in zip(line_pattern.split(), line_syl):\n",
    "        #print(\"I'm here:\", pos, count)\n",
    "        word = get_word(pos,count, prev_word)\n",
    "        l += word + ' '\n",
    "        prev_word = word\n",
    "    return l\n",
    "\n",
    "#This function establishs the parts of speech patterns and syllable counts for the haiku\n",
    "# it returns a list of 3 tuples, for example:\n",
    "# [['NN DT NN NN NN ', [2, 1, 1, 1]], ['NN NN TO DT NN ', [2, 1, 1, 1, 2]], ['NN DT NN MD ', [2, 1, 1, 1]]]\n",
    "def get_line_info():\n",
    "    line_info =[]\n",
    "    for i in range(3):\n",
    "        if i != 1:\n",
    "            tempPat=random.choice(pos_patterns5)\n",
    "            line_info.append([tempPat, random.choice(pattern_syl5[tempPat])])\n",
    "        else:\n",
    "            tempPat=random.choice(pos_patterns7)\n",
    "            line_info.append([tempPat, random.choice(pattern_syl7[tempPat])])\n",
    "    return line_info\n",
    "\n",
    "#This function will print out several haiku\n",
    "def generate_spamku(num_wanted):\n",
    "    try:\n",
    "        for i in range(num_wanted):\n",
    "            line_info = get_line_info()\n",
    "            for line in line_info:\n",
    "                print(make_line(line[0], line[1]))\n",
    "            print()\n",
    "    except:\n",
    "        print(\"\\nError.  Restarting...\\n\")\n",
    "        generate_spamku(num_wanted-i)\n",
    "        pass\n",
    "              \n",
    "\n",
    "generate_spamku(5)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions used to create pickled dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make pos patterns from haiku\n",
    "# pos_patterns =  defaultdict(list)\n",
    "# pos_patterns_markov_data = []\n",
    "# haiku_pat = []\n",
    "# for haiku in haiku_list:\n",
    "#     for line in haiku:\n",
    "#         l=\"\"\n",
    "#         line = line.strip()\n",
    "#         for word in line.split():\n",
    "#             word = clean_word(word)\n",
    "#             try:\n",
    "#                 l += pos_tag([word])[0][1]+\" \"\n",
    "#             except:\n",
    "#                 #I discovered that word is \"\" sometimes, which raises an error\n",
    "#                 #print(\"error when building l\", word)\n",
    "#                 pass\n",
    "#         haiku_pat.append(l)\n",
    "#         if l in pos_patterns:\n",
    "#             pos_patterns[l] += 1\n",
    "#         else:\n",
    "#             pos_patterns[l] = 1\n",
    "#     pos_patterns_markov_data.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make pos pattern - syllable information dict\n",
    "# pos_patterns5 =  []\n",
    "# pos_patterns7 =  []\n",
    "# pattern_syl5 = defaultdict(list)\n",
    "# pattern_syl7 = defaultdict(list)\n",
    "# #pos_patterns_syl_markov_data_syl = []\n",
    "# for haiku in haiku_list:\n",
    "#     for line in haiku:\n",
    "        \n",
    "#         l=\"\"\n",
    "#         syl_pattern = []\n",
    "#         line = line.strip()\n",
    "#         for word in line.split():\n",
    "#             word = clean_word(word)\n",
    "#             try:\n",
    "#                 l += pos_tag([word])[0][1]+\" \"\n",
    "#                 syl_pattern.append(get_syllable_count(word))\n",
    "#             except:\n",
    "#                 #I discovered that word is \"\" sometimes, which raises an error\n",
    "#                 #print(\"error when building l\", word)\n",
    "#                 pass\n",
    "#         if sum(syl_pattern) == 5:\n",
    "#             pattern_syl5[l].append(syl_pattern)\n",
    "#             pos_patterns5.append(l)\n",
    "#         if sum(syl_pattern) == 7:\n",
    "#             pattern_syl7[l].append(syl_pattern)\n",
    "#             pos_patterns7.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 ive\n",
      "10000 to\n",
      "15000 bill\n",
      "20000 western\n",
      "25000 below\n",
      "30000 deadly\n",
      "35000 ate\n",
      "40000 godzilla\n",
      "45000 was\n",
      "50000 their\n",
      "55000 the\n",
      "60000 rises\n",
      "65000 love\n",
      "70000 were\n",
      "75000 stock\n",
      "80000 stronger\n",
      "85000 pan\n",
      "90000 a\n",
      "95000 an\n",
      "100000 i\n",
      "105000 ammo\n",
      "110000 blend\n",
      "***************  Done  ***************\n"
     ]
    }
   ],
   "source": [
    "# # Make pos_dict from spamku\n",
    "\n",
    "# import string\n",
    "# import sys\n",
    "# c=0\n",
    "# pos_dict=defaultdict(list)\n",
    "# for word in text.split(' '):\n",
    "#     try:\n",
    "#         c+=1\n",
    "#         word = clean_word(word) \n",
    "#         if c % 5000 == 0:\n",
    "#             print(c, word)\n",
    "#         tok = pos_tag([word])\n",
    "#         # This line is here to throw an error if we don't have a syllable count for this word\n",
    "#         syl = get_syllable_count(word)\n",
    "#         pos_dict[tok[0][1]].append(tok[0][0])\n",
    "#     except:\n",
    "#         #print(\"SYL ERROR\", tok, word)\n",
    "#         continue\n",
    "# print('***************  Done  ***************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pos_dict, open(\"pos_dict.p\", \"wb\"))\n",
    "pickle.dump(pos_patterns, open(\"pos_patterns.p\", \"wb\"))\n",
    "pickle.dump(pos_patterns5, open(\"pos_patterns5.p\", \"wb\"))\n",
    "pickle.dump(pos_patterns7, open(\"pos_patterns7.p\", \"wb\"))\n",
    "pickle.dump(pattern_syl5, open(\"pattern_syl5.p\", \"wb\"))\n",
    "pickle.dump(pattern_syl7, open(\"pattern_syl7.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##code to remove duplicates from the rejected words list\n",
    "# f=open(\"rejected_words.py\", 'r')\n",
    "# rejects=[]\n",
    "# for word in f:\n",
    "#     word = clean_word(word)\n",
    "#     rejects.append(word)\n",
    "# rejects=set(rejects)\n",
    "# f.close()\n",
    "# f=open(\"clean_rejects.txt\", 'a')\n",
    "# for word in rejects:\n",
    "#     f.write(word.strip()+':  ,\\n')\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## legacy code\n",
    "\n",
    "# pattern_prob = []\n",
    "# for k,v in pos_patterns.items():\n",
    "#     for i in range(v):\n",
    "#         try:\n",
    "#             if v != \"\":\n",
    "#                 pattern_prob.append(k)\n",
    "#         except:\n",
    "#             print(\"error: \", k, v)\n",
    "# num_diff_patterns = len(pattern_prob)\n",
    "\n",
    "# syllable_lengths = {\n",
    "#     'VB': 1,\n",
    "#     'DT': 1,\n",
    "#     'RB': 2,\n",
    "#     'NN': 5,\n",
    "#     'VBG': 3,\n",
    "#     'IN': 1,\n",
    "#     'NNS': 4,\n",
    "#     'PRP$': 1,\n",
    "#     'CC': 1,\n",
    "#     'JJ': 2,\n",
    "#     'WP': 1,\n",
    "#     'VBZ': 1,\n",
    "#     'PRP': 1, \n",
    "#     'VBN': 2,\n",
    "#     'JJS': 2,\n",
    "#     'MD': 1, \n",
    "#     'TO': 1,\n",
    "#     'VBD': 2,\n",
    "#     'VBP': 2,\n",
    "#     'WRB': 1,\n",
    "#     'CD': 1, #cardinal digit, could be more\n",
    "#     'RBR': 2,\n",
    "#     'JJR': 2,\n",
    "#     'WDT': 1,\n",
    "#     'WP$': 1\n",
    "# }\n",
    "\n",
    "\n",
    "# # Maybe do a probability analysis on these?\n",
    "# # CC coordinating conjunction\n",
    "# # CD cardinal digit\n",
    "# # DT determiner\n",
    "# # EX existential there (like: “there is” … think of it like “there exists”)\n",
    "# # FW foreign word\n",
    "# # IN preposition/subordinating conjunction\n",
    "# # JJ adjective ‘big’\n",
    "# # JJR adjective, comparative ‘bigger’\n",
    "# # JJS adjective, superlative ‘biggest’\n",
    "# # LS list marker 1)\n",
    "# # MD modal could, will\n",
    "# # NN noun, singular ‘desk’\n",
    "# # NNS noun plural ‘desks’\n",
    "# # NNP proper noun, singular ‘Harrison’\n",
    "# # NNPS proper noun, plural ‘Americans’\n",
    "# # PDT predeterminer ‘all the kids’\n",
    "# # POS possessive ending parent’s\n",
    "# # PRP personal pronoun I, he, she\n",
    "# # PRP$ possessive pronoun my, his, hers\n",
    "# # RB adverb very, silently,\n",
    "# # RBR adverb, comparative better\n",
    "# # RBS adverb, superlative best\n",
    "# # RP particle give up\n",
    "# # TO, to go ‘to’ the store.\n",
    "# # UH interjection, errrrrrrrm\n",
    "# # VB verb, base form take\n",
    "# # VBD verb, past tense took\n",
    "# # VBG verb, gerund/present participle taking\n",
    "# # VBN verb, past participle taken\n",
    "# # VBP verb, sing. present, non-3d take\n",
    "# # VBZ verb, 3rd person sing. present takes\n",
    "# # WDT wh-determiner which\n",
    "# # WP wh-pronoun who, what\n",
    "# # WP$ possessive wh-pronoun whose\n",
    "# # WRB wh-abverb where, when\n",
    "\n",
    "# def choose_target_syllable_count(pattern, number_of_syllables_wanted):\n",
    "#     syllable_counts = []\n",
    "#     possible = False\n",
    "#     pattern_len = len(pattern.split())\n",
    "#     max_per_word = number_of_syllables_wanted - pattern_len\n",
    "#     for i in range(len(pattern.split())):\n",
    "#         while possible == False:\n",
    "#             x = randint(1, syllable_lengths[pattern.split()[i]])\n",
    "#             print(x)\n",
    "#             if x <= max_per_word:\n",
    "#                 possible = True\n",
    "#         syllable_counts.append(x)\n",
    "#         number_of_syllables_wanted -= x\n",
    "#         pattern_len -= 1\n",
    "#         max_per_word = number_of_syllables_wanted - pattern_len\n",
    "#         print('***********************', syllable_counts)\n",
    "#     return syllable_counts\n",
    "\n",
    "# #pass in poem, pattern, \n",
    "                    \n",
    "# def build_line(line, pattern, max_length):\n",
    "#     if len(pattern) == 0:\n",
    "#         return line\n",
    "#     else:\n",
    "#         pos = pattern[-1]\n",
    "#     try:\n",
    "#         new_max_length = max_length\n",
    "#         #print(max_length)\n",
    "#         word = random.choice(pos_dict[pos])\n",
    "#         new_max_length = max_length - get_syllable_count(word)\n",
    "#         if new_max_length >= 0:\n",
    "#             line = word + ' ' + line\n",
    "#         else:\n",
    "#             new_max_length = max_length\n",
    "#     except:\n",
    "#         print(\"funny word: \", word)        \n",
    "#     #if max_length >= 0:\n",
    "#     pattern=pattern[:-1]\n",
    "#     return build_line(line, pattern, new_max_length)\n",
    "\n",
    "## word1.capitalize()        \n",
    "        \n",
    "# line1text=\"\"\n",
    "# word=\"\"\n",
    "# c=0\n",
    "# for pos in line1:\n",
    "#     word=\"\"\n",
    "#     possible = False\n",
    "#     try:\n",
    "#         while possible == False:\n",
    "#             word = random.choice(pos_dict[pos])\n",
    "#             if get_syllable_count(word) = l1[c]\n",
    "#                 line1text += word \" \"\n",
    "#                 c += 1\n",
    "#                 possible = true\n",
    "#         except:\n",
    "#             print(\"Unexpected error when adding to dict:\", sys.exc_info()[0])\n",
    "#             print(\"pos:\", pos, l1[c], word)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
