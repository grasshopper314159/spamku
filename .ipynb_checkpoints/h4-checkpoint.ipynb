{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\njayj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\njayj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\njayj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk;\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('cmudict')\n",
    "from nltk.corpus import cmudict\n",
    "import string\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "l=[]\n",
    "haiku_list=[]\n",
    "pos_dict=defaultdict(list)\n",
    "pro = cmudict.dict()\n",
    "def get_word(POS, syllables, preceding_word):\n",
    "    pass\n",
    "\n",
    "def get_syllable_count(word):\n",
    "    #print(pro[word])\n",
    "    syl=[ x for x in pro[word][0] if x[-1].isdigit()]\n",
    "    return len(syl)\n",
    "\n",
    "def clean_word(word):\n",
    "    word= word.translate(str.maketrans('', '', string.punctuation))\n",
    "    word=word.lower()\n",
    "    return word\n",
    "\n",
    "def markov_chain(text):\n",
    "    '''The input is a string of text and the output will be a dictionary with each word as\n",
    "       a key and each value as the list of words that come after the key in the text.'''\n",
    "    \n",
    "    # Tokenize the text by word, though including punctuation\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    # Initialize a default dictionary to hold all of the words and next words\n",
    "    m_dict = defaultdict(list)\n",
    "    \n",
    "    # Create a zipped list of all of the word pairs and put them in word: list of next words format\n",
    "    for current_word, next_word in zip(words[0:-1], words[1:]):\n",
    "        current_word = clean_word(current_word)\n",
    "        next_word = clean_word(next_word)\n",
    "        m_dict[current_word].append(next_word)\n",
    "\n",
    "    # Convert the default dict back into a dictionary\n",
    "    m_dict = dict(m_dict)\n",
    "    return m_dict\n",
    "\n",
    "# def process_one_list(a_list):\n",
    "#     t=\"\"\n",
    "#     for lines in a_list:\n",
    "#         t+=line.strip()+' '\n",
    "#     return t\n",
    "\n",
    "# def process_list_of_lists(lists):\n",
    "#     t=\"\"\n",
    "#     for lines in lists:\n",
    "#         t = process_one_list(lines)\n",
    "#     return t\n",
    "\n",
    "\n",
    "# This file contains 8893 spam-ku\n",
    "filename=\"corpus/spamku.txt\"\n",
    "file = open(filename, mode = 'r')\n",
    "#text=file.read()\n",
    "text = \"\"\n",
    "\n",
    "for line in file:\n",
    "    text += line.strip()+' '\n",
    "file.close()\n",
    "mc=markov_chain(text)\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_sentence(chain, count=15):\n",
    "    '''Input a dictionary in the format of key = current word, value = list of next words\n",
    "       along with the number of words you would like to see in your generated sentence.'''\n",
    "\n",
    "    # Capitalize the first word\n",
    "    word1 = random.choice(list(chain.keys()))\n",
    "    sentence = word1.capitalize()\n",
    "\n",
    "    # Generate the second word from the value list. Set the new word as the first word. Repeat.\n",
    "    for i in range(count-1):\n",
    "        word2 = random.choice(chain[word1])\n",
    "        word1 = word2\n",
    "        sentence += ' ' + word2\n",
    "\n",
    "    # End it with a period\n",
    "    sentence += '.'\n",
    "    return(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Drunk girls check flags hormel delivery man i like a pet dog no spray spamthe.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(mc, count=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S', 'P', 'AE1', 'M']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_syllable_count('spam')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not really using haiku list now\n",
    "#it makes a list of haiku, so each item in the list is a complete three line haiku\n",
    "haiku_list=[]\n",
    "filename=\"corpus/spamku.txt\"\n",
    "file = open(filename, mode = 'r')\n",
    "# make a list of spamku\n",
    "for line in file:\n",
    "    c+=1\n",
    "    l.append(line)\n",
    "    if c==3:\n",
    "        haiku_list.append(l)\n",
    "        c=0\n",
    "        l=[]\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Flying', 'VBG'), ('through', 'IN'), ('the', 'DT'), ('bedroom', 'NN'), ('door', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# from nltk import pos_tag\n",
    "# tokens_tag = haiku_list[0][1].split()\n",
    "# tokens_tag2 = pos_tag(tokens_tag)\n",
    "# print(tokens_tag2)\n",
    "# for tup in tokens_tag2:\n",
    "#     pos_dict[tup[1]].append(tup[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spam', 'NN')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(['spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flying through the bedroom door'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haiku_list[0][1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 in\n",
      "2000 mail\n",
      "3000 symbol\n",
      "4000 atom\n",
      "5000 ive\n",
      "6000 can\n",
      "7000 freakin\n",
      "8000 god\n",
      "9000 long\n",
      "10000 to\n",
      "11000 have\n",
      "12000 and\n",
      "13000 knees\n",
      "14000 weight\n",
      "15000 bill\n",
      "16000 spam\n",
      "17000 spam\n",
      "18000 blue\n",
      "19000 his\n",
      "20000 western\n",
      "21000 spam\n",
      "22000 of\n",
      "23000 in\n",
      "24000 unavailable\n",
      "25000 below\n",
      "26000 mama\n",
      "27000 wafers\n",
      "28000 the\n",
      "29000 pinkie\n",
      "30000 deadly\n",
      "31000 than\n",
      "32000 compute\n",
      "33000 love\n",
      "34000 the\n",
      "35000 ate\n",
      "36000 still\n",
      "37000 use\n",
      "38000 pink\n",
      "39000 pants\n",
      "40000 godzilla\n",
      "41000 real\n",
      "42000 bits\n",
      "43000 i\n",
      "44000 west\n",
      "45000 was\n",
      "46000 swine\n",
      "47000 into\n",
      "48000 dont\n",
      "49000 stamps\n",
      "50000 their\n",
      "51000 little\n",
      "52000 the\n",
      "53000 through\n",
      "54000 smoothie\n",
      "55000 the\n",
      "56000 hands\n",
      "57000 cans\n",
      "58000 feel\n",
      "59000 tin\n",
      "60000 rises\n",
      "61000 have\n",
      "62000 know\n",
      "63000 you\n",
      "64000 if\n",
      "65000 love\n",
      "66000 slice\n",
      "67000 you\n",
      "68000 spam\n",
      "69000 glorious\n",
      "70000 were\n",
      "71000 sales\n",
      "72000 she\n",
      "73000 to\n",
      "74000 oval\n",
      "75000 stock\n",
      "76000 the\n",
      "77000 pleasure\n",
      "78000 my\n",
      "79000 and\n",
      "80000 stronger\n",
      "81000 of\n",
      "82000 will\n",
      "83000 dead\n",
      "84000 im\n",
      "85000 pan\n",
      "86000 with\n",
      "87000 spam\n",
      "88000 spam\n",
      "89000 disturbs\n",
      "90000 a\n",
      "91000 one\n",
      "92000 is\n",
      "93000 sinking\n",
      "94000 it\n",
      "95000 an\n",
      "96000 but\n",
      "97000 the\n",
      "98000 its\n",
      "99000 when\n",
      "100000 i\n",
      "101000 powerless\n",
      "102000 crazy\n",
      "103000 and\n",
      "104000 earl\n",
      "105000 ammo\n",
      "106000 conspiracy\n",
      "107000 pickle\n",
      "108000 spam\n",
      "109000 for\n",
      "110000 blend\n",
      "111000 i\n",
      "112000 served\n",
      "113000 and\n",
      "***************  Done  ***************\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import sys\n",
    "c=0\n",
    "for word in text.split(' '):\n",
    "    c+=1\n",
    "    word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "    word = word.lower()\n",
    "    \n",
    "    if c % 5000 == 0:\n",
    "        print(c, word)\n",
    "    try:\n",
    "        tok = pos_tag([word])\n",
    "    except:\n",
    "        pass\n",
    "        #I discovered that word is \"\" often which raises an error\n",
    "#         print(\"Unexpected error when using pos_tag:\", sys.exc_info()[0])\n",
    "#         print(\"Word: \", word)\n",
    "    try:\n",
    "        #print(tok)\n",
    "        pos_dict[tok[0][1]].append(tok[0][0])\n",
    "    except:\n",
    "        # Haven't seen any of these\n",
    "        print(\"Unexpected error when adding to dict:\", sys.exc_info()[0])\n",
    "        print(tok)\n",
    "print('***************  Done  ***************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['VB', 'DT', 'RB', 'NN', 'VBG', 'IN', 'NNS', 'PRP$', 'CC', 'JJ', 'WP', 'VBZ', 'PRP', 'VBN', 'JJS', 'MD', 'TO', 'VBD', 'VBP', 'WRB', 'CD', 'RBR', 'JJR', 'WDT', 'WP$'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_dict2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4138"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_dict['VBG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump( favorite_color, open( \"save.p\", \"wb\" ) )\n",
    "# favorite_color = pickle.load( open( \"save.p\", \"rb\" ) )\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "#**************  spamku_gen.py has the sortwords_by_syllable function now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pos_dict, open(\"pos_dict.p\", \"wb\"))\n",
    "#pickle.dump(pos_dict, open(\"pos_dict.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filename): \n",
    "    # for reading also binary mode is important \n",
    "    file = open(filename, 'rb')      \n",
    "    data = pickle.load(file) \n",
    "    file.close()\n",
    "    return data\n",
    "pos2=loadData('pos_dict.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['VB', 'DT', 'RB', 'NN', 'VBG', 'IN', 'NNS', 'PRP$', 'CC', 'JJ', 'WP', 'VBZ', 'PRP', 'VBN', 'JJS', 'MD', 'TO', 'VBD', 'VBP', 'WRB', 'CD', 'RBR', 'JJR', 'WDT', 'WP$'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See\n",
      "the\n",
      "pretty\n",
      "SPAM\n",
      "VB DT RB NN \n",
      "Flying\n",
      "through\n",
      "the\n",
      "bedroom\n",
      "door\n",
      "VBG IN DT NN NN \n",
      "Heading\n",
      "towards\n",
      "your\n",
      "face.\n",
      "VBG NNS PRP$ NN \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-ab3c6ef1e7e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mpos_patterns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mpos_patterns_markov_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhaiku_pat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhaiku_pat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Make pos patterns from haiku\n",
    "pos_patterns =  defaultdict(list)\n",
    "pos_patterns_markov_data = defaultdict(list)\n",
    "haiku_pat = []\n",
    "for haiku in haiku_list:\n",
    "    for line in haiku:\n",
    "        l=\"\"\n",
    "        line = line.strip()\n",
    "        #print(line)\n",
    "        for word in line.split():\n",
    "            word = clean_word(word)\n",
    "            try:\n",
    "                l += pos_tag([word])[0][1]+\" \"\n",
    "            except:\n",
    "                print(\"error when building l\", word)\n",
    "        haiku_pat.append(l)\n",
    "        print(l)\n",
    "        if l in pos_patterns:\n",
    "            pos_patterns[l] += 1\n",
    "        else:\n",
    "            pos_patterns[l] = 1\n",
    "    pos_patterns_markov_data[haiku_pat].append(haiku_pat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VB ',\n",
       " 'VB DT ',\n",
       " 'VB DT RB ',\n",
       " 'VB DT RB NN ',\n",
       " 'VBG ',\n",
       " 'VBG IN ',\n",
       " 'VBG IN DT ',\n",
       " 'VBG IN DT NN ',\n",
       " 'VBG IN DT NN NN ',\n",
       " 'VBG ',\n",
       " 'VBG NNS ',\n",
       " 'VBG NNS PRP$ ',\n",
       " 'VBG NNS PRP$ NN ']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haiku_pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag([word])[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
